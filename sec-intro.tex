\def\ignore#1{}
\section{Introduction}

Information Technologies have been transforming education dramatically
recently, leading to the rapid growth of the Massive Open Online Courses
(MOOCs), which have not only made education more affordable and scalable,
but also have huge potential to enable more effective personalized
learning.  A key component technology that enabled the success of MOOCs is
the automatic grading capability of a MOOC system. Unfortunately, the
current technology for automatic grading is mostly limited to multi-choice
questions, short answers~\cite{Brooks:2014:Powergrading,
Leacock:2003:CatH, Mitchell:2002:ICAA, Pulman:2005:EdAppsNLP,
Mohler:2009:EACL}, and simple essay scoring~\cite{Chen:2014:IRRODL,
Balfour:2013}, which makes it impossible for the current MOOCs to provide
sophisticated assignments that are needed for teaching complex concepts or
skills (e.g., critical thinking skills) since they cannot be easily graded
in a scalable way. A solution currently adopted to bypass this difficulty
is to use the calibrated peer review~\cite{Balfour:2013, Chen:2014:IRRODL,
Sandeen:2013, Suen:2014}.  However, there are systematic problems with this
approach: discrepancy between peer and instructor ratings, variation in
ratings over time by the same peer rater, inconsistency across exercises
for rating two works of similar quality, differences in rater stringency,
and random fluctuation of ratings of the same work under varied
conditions~\cite{Suen:2014}.  Preliminary data from a recent attempt to use
this technique with veterinary students has also shown that peer reviews
have a distinct positive bias (vide infra) relative to an expert instructor
analysis~\cite{Ferguson:2014}.  Thus it is important to develop more
powerful automatic grading technology that can be applied to more
sophisticated exercises than those provided by the current MOOCs, which are
necessary in many education scenarios.

To automate grading of such a complex assignment, a natural idea is to use
supervised machine learning to learn from the graded examples for
automatically assigning grades to those ungraded ones. As in other machine
learning applications, the general idea here is that if we can extract
those features from the assignments that can indicate the quality of an
assignment, a machine learning program would be able to pick up the
patterns of the features that can distinguish high-quality work from
low-quality work from a sample of graded assignments (i.e., ``training
data''), thus potentially assigning a grade automatically to an ungraded
assignment.

Although this idea is natural and appealing, however, there are many challenges and
questions that we must address before we can effectively deploy such a
technology in a real education environment, and a main goal of this paper
is to take a first step toward systematically addressing these questions,
including specifically the following questions:

\begin{enumerate}
\item {\bf Feasibility:} How feasible is it to use machine learning to automate grading of a complex assignment? 
What general features can we extract from assignments for automated
 grading? How effective are the state of the art machine learning approaches
 for automated grading? Are they sufficiently effective to be immediately
 useful in practice?

\item {\bf Problem Formulation and Evaluation:}  How should we formulate the grading problem as a machine learning
 problem? There are at least two options. One is to frame it as a
 classification problem with the goal of classifying an assignment into one
 of the finite number of pre-defined grade levels based on the rubrics. The
 other is to frame it as a ranking problem where the goal is to rank the
 assignments based on the quality without necessarily assigning a specific
 level of grade, and human graders can then go through the ranked list to
 segment the assignments into different grade levels. How should we design evaluation metrics
to measure the quality of the results of automated grading? 

\item {\bf Integration of Automated Grading and Human Grading:} How exactly should such an automated grader be integrated with
 instructor or TA grading? A more general question is: how can we optimize
 the collaboration of an imperfect automated grader with more reliable
 human graders? Intuitively, the optimization depends on a trade-off between
 the quality/reliability of grading and the amount of human effort
 required. But given an expected amount of human effort, what is the best
 way to have the automated grader to assist human in grading, and to have
 humans to help train the machine learning-based automated grader?

\end{enumerate}

We will study these questions using a particular type of complex
assignments that require sophisticated critical thinking skills, i.e.,
medical case assessment. This kind of assignments are very important for
medical professional education. Thus by studying how to automate grading
for medical assessment assignments, we can potentially enable medical
professional education to scale up, a much needed effort. 
\ignore{
Traditional
veterinary curricula have usually consisted of didactic lectures for as
long as 3 years before entry into the clinic. In veterinary medicine, this
approach has been shown to result in an actual decline in critical thinking
skills~\cite{Herron:1992}. Therefore, to encourage self-awareness and
lifelong learning, didactic instruction should be supplemented by practice
with the process of information gathering and critical clinical thinking
associated with diagnosis of disease and selection of treatment. In her
book on critical thinking in clinical practice~\cite{Gambrill:2005},
Gambrill notes that evidence-based practice is rooted in a willingness to
recognize the intrinsic uncertainty of clinical decision-making.
%Not all clinicians are willing to acknowledge
%this uncertainty in their mission to help patients. This deficiency in
%training in critical clinical thinking is thus not unique to our veterinary
%curricula; in fact,
Furthermore, 
}
Not teaching clinicians about clinical uncertainty has
been referred to as ``the greatest deficiency of medical education
throughout the twentieth century''~\cite{Djulbegovic:2004, Gambrill:2005}.
\ignore{
Evidence-based medicine (EBM) requires the skillset to develop answerable
questions relevant to a case, and to answer these questions with an honest
and open appraisal of research findings~\cite{Braddock:1999}.
Learning by doing is emphasized in EBM and evaluation of case
studies provides such practice. Instructors need to be prepared to provide
structure to the process together with timely feedback during practice in
order to be effective~\cite{Gambrill:2005}. }
However, implementing an instruction plan with an online education system at large scale
to teach clinical uncertainty in decision making raises many significant challenges that must be solved, particularly
challenges in automatic evaluation of the case studies completed by the
students, which we address in this paper by leveraging information
retrieval and machine learning techniques.

To study the feasibility questions, we propose a general methodology for designing three
complementary types of feature representations of such complex assignments,
including token features, similarity features, and selection features, and experiment
with these features using ordinal regression for predicting the grade levels in multiple
dimensions of rubrics.  The
token features are based on the term tokens extracted from an assignment
and they offer the most general and robust representation. The similarity
features are to capture the similarity between an assignment and the
solution provided by an instructor; the intuition is that the higher the
similarity is, the higher the grade should be.  Finally, the selection
features are to quantify the accuracy of the selection of relevant parts in
a case description based on how well the selected parts match the
solutions.  While it is generally beneficial to manually design
assignment-specific features, such features cannot be generalized to work
on other assignments, in this paper, we focus on studying {\em general}
features that can be {\em automatically computed} on any semi-structured
complex assignments, and aim at understanding their effectiveness.

A practical challenge in studying our problem is the lack of a large set of
graded assignments which is needed both for training a machine learning
program and for validating the results of automated grading. This is partly
due to the fact that grading such assignments takes much human effort, the
very reason why we need to study automated grading for such assignments. In
our experiments, we used a data set of 107 student submissions for one
medical case assessment assignment that are available to us. While the data
set is small, we were able to observe statistically significant differences
in our experiments, thus it still allows us to draw meaningful conclusions
about different approaches to automated grading. 

Our study with this data set shows that it is feasible to automate grading of 
a complex assignment such as medical case assessment using standard
 machine learning approaches and the proposed three kinds of general features 
provided the instructor can grade a small number of examples, but 
the grading accuracy on different rubric categories vary substantially. 

The results of our feasibility study reveal that there is a great deal of 
variation in the grades given by instructorrs due to the inevitable subjectivity of the rubrics. 
This suggests that  it might be less effort and more reliable for an instructor to make pairwise judgments between
a pair of assignments  as opposed to assigning an exact numerical or letter grade. Working 
on such pairwise preference judgments also makes it easy to integrate
 non-expert judgments (such as peer grading) that might not be reliable in the exact grades assigned but may include relatively reliable preference judgments. Moreover, working on pairwise preferences naturally ``eliminate" the need for normalizing numerical grades which might be biased 
(e.g., some graders may be overly generous).  
%as well  imagine having peer graders provide the higher-accuracy, bias-free "which is better" judgments rather than having to assign their peers a specific grade. These judgments can then be used as supervision, perhaps with different instance weights than if they were coming from an instructor.
Given that we will attempt to obtain pairwise preferences as training examples, it follows that we should frame the problem of automated grading as to rank the ungraded assignments, as opposed to predict the exact grade of an assignment. The ranking would be in descending order of quality (in any rubric dimension or overall quality with consideration of multiple dimensions), and a human grader can  then easily segment the list into any desired grade levels.  In comparison with predicting exact grades, such a ranking formulation also offers a natural way to engage humans in validating and finalizing the grades. 
%In comparison with directly assigning a grade level to each assignment using machine learning, such a ranking
%strategy has several advantages: .... {\bf [list the benefits or our motivation  here]}. 
For evaluation, althoough retrieval measures such as Mean Average Precision (MAP) or normalized Discounted Cumulative Gain (nDCG) are commonly used for evaluating a ranked list, we suggest that the Normalized Distance-based Performance Measure (NDPM) is a better measure for our ranking problem since it can better handle many ties that inevitably exist in our case. 

In practice, an automated grader must be integrated with a human grader so as to minimize the overall effort of the human grader while ensuring a certain level of grading accuracy. There is an inherent tradeoff here since to increase the grading accuracy, we would like to have as many training examples (i.e., manually graded assignments) as possible, which, however, would incur more human effort. To optimize human-machine collaboration and enable a flexible tradeoff between human effort and grading accuracy, we propose the following sequential
training process based on active machine learning: 1) human grader first grades a small number of assignments as the initial training set; 2) the machine would learn from the intial set, and identify the next ``best" example (i.e., assignment) to label and present it for human to grade where ``best" means that the example is most valuable to help the automated grader improve its accuracy; 3) human would grade the nominated example to increase the size of the training set by one; 4) machine would learn from the augmented training set and repeatedly present a new example for human to grade until it reaches a desired level of accuracy, at which point, the process stops and the human grader would segment the final ranked list to generate grades for all the assignments. Our experiment results show that such an active learning process is much more effective than batch training. 

As the first study of this new problem, our study is inevitably limited by
the size of the data set due to the unavailability of larger data sets; as the automated grading technology matures for
such assignments, we anticipate to have much larger data sets available to
further verify our observations and conclusions.

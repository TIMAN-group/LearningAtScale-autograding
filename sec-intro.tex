\def\ignore#1{}
\section{Introduction}

Information Technologies have been transforming education dramatically
recently, leading to the rapid growth of Massive Open Online Courses
(MOOCs), which have not only made education more affordable and scalable,
but also have huge potential to enable more effective personalized
learning.  Automatic grading technology has been a key component enabling
the success of MOOCs. Unfortunately, the
current technology for automatic grading is mostly limited to multiple-choice
questions, short answers~\cite{Brooks:2014:Powergrading,
Leacock:2003:CatH, Mitchell:2002:ICAA, Pulman:2005:EdAppsNLP,
Mohler:2009:EACL}, and simple essay scoring~\cite{Balfour:2013}, which
makes it quite challenging for the current MOOCs to provide sophisticated
assignments for teaching complex concepts or skills (e.g., critical
thinking skills) since they cannot be easily graded in a scalable way. A
solution currently adopted to bypass this difficulty is to use the
calibrated peer review~\cite{Balfour:2013, Suen:2014, Piech:2013:EDM}.
While there are encouraging findings about peer assessment and methods
proposed to improve it~\cite{Kulkarni:2013, Piech:2013:EDM}, there are
still systematic problems with this approach: discrepancy between peer and
instructor ratings, variation in ratings over time by the same peer rater,
inconsistency across exercises for rating two works of similar quality,
differences in rater stringency, and random fluctuation of ratings of the
same work under varied conditions~\cite{Suen:2014}. Preliminary data from a
recent attempt to use this technique with veterinary students has also
shown that peer reviews have a distinct positive bias (vide infra) relative
to an expert instructor analysis~\cite{Ferguson:2014}. Thus, it is
important to develop more powerful automatic grading technology that can be
applied to more sophisticated exercises than those provided by the current
MOOCs, which are necessary in many education scenarios.

To automate grading of such a complex assignment, a natural idea is to use
supervised machine learning to learn from graded examples for automatically
assigning grades to ungraded assignments. As in other machine learning
applications, the general idea here is that if we can extract those
features from the assignments that can indicate the quality of an
assignment, a machine learning program would be able to pick up the
patterns of the features that can distinguish high-quality work from
low-quality work from a sample of graded assignments (i.e., ``training
data''), thus potentially assigning a grade automatically to an ungraded
assignment.

Although this idea is natural and appealing, there are many challenges and
questions that we must address before we can effectively deploy such a
technology in a real education environment, and a main goal of this paper
is to take a first step toward systematically addressing these questions.

\begin{enumerate}
  \item {\bf Feasibility:} How feasible is it to use machine learning to
    automate grading of a complex assignment?  What general features can we
    extract from assignments for automated grading? How effective are the
    state of the art machine learning approaches for automated grading? Are
    they sufficiently effective to be immediately useful in practice?

  \item {\bf Problem Formulation and Evaluation:}  How should we formulate
    the grading problem as a machine learning problem? There are at least
    two options. One is to frame it as a classification problem with the
    goal of classifying an assignment into one of the finite number of
    pre-defined grade levels based on a rubric. The other is to frame it as
    a ranking problem where the goal is to rank the assignments based on
    the quality without necessarily assigning a specific grade---human
    graders can then go through the ranked list to segment the assignments
    into different grade levels. How should we design evaluation metrics to
    measure the quality of the results of automated grading?

  \item {\bf Integration of Automated Grading and Human Grading:} How
    exactly should such an automated grader be integrated with instructor
    or TA grading? A more general question is: how can we optimize the
    collaboration of an imperfect automated grader with more reliable human
    graders? Intuitively, the optimization depends on a trade-off between
    the quality/reliability of grading and the amount of human effort
    required. But given an expected amount of human effort, what is the
    best way to have the automated grader to assist a person in grading?
    What is the best way to have a human grader help train the
    machine-learning based automated grader?
\end{enumerate}

We will study these questions using a particular type of complex
assignment that requires sophisticated critical thinking skills, i.e.,
medical case assessment. This kind of assignment is very important for
medical professional education. By studying how to automate grading
for medical assessment assignments, we can potentially enable medical
professional education to scale up---a much needed effort.
\ignore{
Traditional
veterinary curricula have usually consisted of didactic lectures for as
long as 3 years before entry into the clinic. In veterinary medicine, this
approach has been shown to result in an actual decline in critical thinking
skills~\cite{Herron:1992}. Therefore, to encourage self-awareness and
lifelong learning, didactic instruction should be supplemented by practice
with the process of information gathering and critical clinical thinking
associated with diagnosis of disease and selection of treatment. In her
book on critical thinking in clinical practice~\cite{Gambrill:2005},
Gambrill notes that evidence-based practice is rooted in a willingness to
recognize the intrinsic uncertainty of clinical decision-making.
%Not all clinicians are willing to acknowledge
%this uncertainty in their mission to help patients. This deficiency in
%training in critical clinical thinking is thus not unique to our veterinary
%curricula; in fact,
Furthermore, 
}
Not teaching clinicians about clinical uncertainty has been referred to as
``the greatest deficiency of medical education throughout the twentieth
century''~\cite{Djulbegovic:2004, Gambrill:2005}.
\ignore{
Evidence-based medicine (EBM) requires the skillset to develop answerable
questions relevant to a case, and to answer these questions with an honest
and open appraisal of research findings~\cite{Braddock:1999}.
Learning by doing is emphasized in EBM and evaluation of case
studies provides such practice. Instructors need to be prepared to provide
structure to the process together with timely feedback during practice in
order to be effective~\cite{Gambrill:2005}. }
However, implementing an instruction plan with an online education system
at large scale to teach clinical uncertainty in decision making raises many
significant challenges that must be solved, particularly challenges in
automatic evaluation of the case studies completed by the students, which
we address in this paper by leveraging information retrieval and machine
learning techniques.

To study the feasibility questions, we propose a general methodology for
designing three complementary types of feature representations of such
complex assignments, including token features, similarity features, and
selection features, and experiment with these features using ordinal
regression for predicting the grade levels in multiple dimensions of
rubrics. The token features are based on the term tokens extracted from an
assignment and they offer the most general representation and are robust in
practice. The similarity features are to capture the similarity between an
assignment and the solution provided by an instructor; the intuition is
that the higher the similarity is, the higher the grade should be.
Finally, the selection features are to quantify the accuracy of the
selection of relevant parts in a case description based on how well the
selected parts match the solutions (e.g., choosing to run the right
lab tests in a clinical case).  While it is generally beneficial to
manually design assignment-specific features, such features cannot be
generalized to work on other assignments; in this paper, we focus on
studying \emph{general} features that can be \emph{automatically computed}
on any semi-structured complex assignment, and aim at understanding their
effectiveness.

A practical challenge in studying our problem is the lack of a large set of
graded assignments which is needed both for training a machine learning
program and for validating the results of automated grading. This is partly
due to the fact that grading such assignments takes much human effort: the
very reason why we need to study automated grading for such assignments. In
our experiments, we used a data set of 107 student submissions for one
medical case assessment assignment that is available to us. While the data
set is small, we are able to observe statistically significant differences
in our experiments, thus it still allows us to draw meaningful conclusions
about different approaches to automated grading.

Our study with this data set shows that it is feasible to automate grading
of a complex assignment such as a medical case assessment using standard
machine learning approaches and the proposed three kinds of general
features provided the instructor can grade a small number of examples, but
the grading accuracy on different rubric categories varies substantially.

The results of our feasibility study reveal that there is a great deal of
variation in the grades given by instructors due to the inevitable
subjectivity of the rubrics. This suggests that it might be less effort
and more reliable for an instructor to make pairwise judgments between a
pair of assignments as opposed to assigning an exact numerical or letter
grade. Working on such pairwise preference judgments also makes it easy to
integrate non-expert judgments (such as peer grading) that might not be
reliable in the exact grades assigned but may include relatively reliable
preference judgments. Moreover, working on pairwise preferences naturally
``eliminates'' the need for normalizing numerical grades which might be
biased (e.g., some graders may be overly generous).

Given that we will attempt to obtain pairwise preferences as training
examples, it follows that we should frame the problem of automated grading
as to rank the ungraded assignments, as opposed to predict the exact grade
of an assignment. The ranking would be in descending order of quality (in
any rubric dimension or overall quality with consideration of multiple
dimensions), and a human grader can  then easily segment the list into any
desired grade levels.  In comparison with predicting exact grades, such a
ranking formulation also offers a natural way to engage humans in
validating and finalizing the grades.  For evaluation, although retrieval
measures such as Mean Average Precision (MAP) or normalized Discounted
Cumulative Gain (nDCG) are commonly used for evaluating a ranked list, we
suggest that the Normalized Distance-based Performance Measure
(NDPM)~\cite{Yao:1995:JASIS} is a better measure for our ranking problem
since it can better handle the many inevitable ties that occur in our case.

In practice, an automated grader must be integrated with a human grader so
as to minimize the overall effort of the human grader while ensuring a
certain level of grading accuracy. There is an inherent trade-off here since
to increase the grading accuracy we would like to have as many training
examples (i.e., manually graded assignments) as possible, which, however,
would incur more human effort. To optimize human-machine collaboration and
enable a flexible trade-off between human effort and grading accuracy, we
propose the following sequential training process based on active machine
learning:
\begin{enumerate*}[label=\itshape(\arabic*)\upshape]
  \item a human grader first grades a small number of assignments as the
    initial training set (this could be either numeric or letter grades, or
    pairwise judgments);
  \item the machine would learn from the initial set, and identify the next
    ``best'' example (i.e., assignment) to label and present it for human
    to grade (where ``best'' here means that the example is most valuable
    to help the automated grader improve its accuracy);
  \item a human would grade the nominated example to increase the size of
    the training set by one;
  \item the machine would learn from the augmented training set and
    repeatedly present a new example for the human to grade until it
    reaches a desired level of accuracy, at which point, the process stops
    and the human grader would segment the final ranked list to generate
    grades for all the assignments.
\end{enumerate*}
Our experiment results show that this active learning process is much
more effective than batch training.

As the first study of this new problem, it is inevitably limited by the
size of the data set due to the unavailability of larger data sets; as the
automated grading technology matures for such assignments, we anticipate to
have much larger data sets available to further verify our observations and
conclusions.

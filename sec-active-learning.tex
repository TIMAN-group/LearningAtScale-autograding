\section{Efficiently Utilizing Human Judgments with Active Learning}
As in all supervised learning approaches, 
the accuracy of  the automated grader based on learning to rank. depends
on the quantity and quality of the training examples available for the 
learner to use. Ideally, we would like human graders to provide
 as many graded examples as possible, but this would
reduce the benefit of an automated grader. Indeed,
if a human grader completes grading all the assignments,
there would be no need for the automated grader!
However, if there are insufficient training examples 
to learn from, the automated grader migh have a low accuracy,
which would further require more human effort on 
``post-editing" the grading results of the automated grader. 
Thus there is clearly a complicated tradeoff between
the effort of manual grading and the utility of the trained
 grader that may have to be empirically optimized in an application-specific way.

However, it is very clear that if we ask human graders to grade
a certain amount of assignments, we would like the
graded assignments to be as useful to the automated grader as possible.
Just randomly selecting a sample of assignments for manual grading
is not the best way. 
\ignore{
A serious drawback of the supervised learning approach is the lack
of early feedback in the instructor--machine collaboration. In the
supervised approach, a model is learned on some subset of the assignments
to predict the grades of the remaining assignments, but the process of
selecting a subset of assignments for grading is not guided. It is possible
that randomly selecting assignments to grade results in mostly redundant
information being given to the grading model. Instead, }
A natural solution to this problem is to employ 
active learning to allow the machine learning model to
guide the instructor in providing the supervision to make the most
effective use of his/her effort.

Building on these observations, we thus propose the following ``pairwise active
learning to rank'' model for automatic grading, which will employ the
following process where $k_1$ is a parameter that can be empirically set:
\begin{enumerate*}[label=\itshape(\arabic*)\upshape]
  \item Ask the instructor for comparative judgments on $k_1$ pairs of
    assignments,

  \item Learn a model using a learning-to-rank approach on the available
    pairwise judgments\label{al:learn-model-step},

  \item Apply the model to all remaining unjudged pairs,

  \item Select an unjudged pair to present to the instructor for
    judgment\label{al:select-training-step}, and

  \item Go to step~\ref{al:learn-model-step}.
\end{enumerate*}
Instantiations of this general approach will differ mainly in
steps~\ref{al:learn-model-step}~and~\ref{al:select-training-step}.

% subsection heading removed for space...
%\subsection{Experiments}

To study whether our proposed active learning approach better utilizes
human judgments during the grading process, we performed the following
experiment. We took our assignments and assigned each a ``composite
score'', computed as the average of their ordinal score for each of the six
rubric dimensions. Our task is then to learn a ranking that is consistent
with the ranking produced by these composite scores while simultaneously
\emph{minimizing instructor effort} in labeling.

We first transform the $n = 107$ assignments into $\frac{1}{2}n(n-1) =
5671$ assignment \emph{pairs} $(x_i, x_j)$ with corresponding labels
$y_{ij} \in \{+1, -1\}$ indicating whether $x_i$ should be ranked above or
below $x_j$ in the ranking. Ties were broken arbitrarily by assignment id.
The supervision given by the instructor is then to indicate a preference
for ranking $x_i$ relative to $x_j$.

Following the process laid out in the beginning of the section, we first
start with $k_1 = 10$ random pairs selected from the transformed data and
ask for labels from the instructor. We then learn the model, compute the
NDPM for the ranking produced by the model for all $n$ assignments, and
then ask for additional supervision by selecting the unlabeled assignment
pair whose distance from the decision boundary for the model is lowest
(this is a known, simple approach to uncertainty
sampling~\cite{Settles:2012}) and repeat the training/evaluation loop. Our
particular model choice was a linear SVM provided through the \textsc{MeTA}
toolkit.

We compare this active learning scenario with a random learning baseline,
which is the exact same process as above, but instead of selecting the most
uncertain pair in the unlabeled data we select one uniformly at random.
This will allow us to see whether the uncertainty sampling approach is
truly helping to guide the learning process to make more efficient
supervision choices or not.

\begin{figure}
  \begin{center}
   \begin{tikzpicture}[scale=0.9]
    \begin{axis}[xlabel={human-labeled pairs},ylabel={NDPM}, xmax=500,
      legend style={font=\small}]

      \addplot+[color=red, each nth point={10}, filter discard
          warning=false, mark=triangle, error bars/.cd, y dir=both, y explicit]
        table [x=training-size, y=AVG-NDPM, y error=std-deviation, col
            sep=comma] {data/random-learning.csv};

      \addplot+[color=blue, each nth point={10}, filter discard
          warning=false, mark=x, error bars/.cd, y dir=both, y explicit]
        table [x=training-size, y=AVG-NDPM, y error=std-deviation, col
          sep=comma] {data/active-learning.csv};

%      \addplot[mark=none, color=green]
%        coordinates {(0, 0.344637) (500, 0.344637)};

      \legend{Random learning, Active learning, BM25 similarity}
    \end{axis}
  \end{tikzpicture}
  \caption{A comparison between a randomized learning solution and an
  active learning solution to the grading-as-ranking problem. Reported is
  the average NDPM (lower is better) over 5 runs, with error bars indicating
  one standard deviation.}
  \label{fig:active-learning}
  \end{center}
\end{figure}

Our results are summarized in Figure~\ref{fig:active-learning}. Recall that
a NDPM value of $0.3$ indicates that a system ranking was 30\% of the
maximal achievable distance away from the reference ranking. We can see
that even at a small fraction of all of the assignment pairs, the active
learning approach (blue line) is able to achieve better NDPM than simply
learning at random (red line). This is consistent with our hypothesis that
active learning as part of an automatic grading system can make more
effective use of an instructor's time than a purely passive supervised
approach.

How much instructor effort goes in to judging 200 assignment pairs? This
may initially seem like a lot, but each pair is not labeled in
isolation---labeling many pairs will inevitably include assignments that
have already been seen before. These familiar assignments make providing a
pairwise judgment faster than it would be if done ``cold''. In general, it
is also reasonable to assume that the effort involved in simply saying
whether assignment $a$ is better than assignment $b$ is lower than having
to consult a rubric to assign an actual point (or letter) value. It is
important, however, to ensure that providing a pairwise judgment takes as
little effort as possible relative to assigning a numeric or letter grade.
An interesting future direction is then to design an interactive system
that attempts to further drive the cost of providing judgments down.

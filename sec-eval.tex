\section{Automated Grading as Ranking Assignments}

Some of the results from our feasibility study using ordinal regression raise
 the question whether framing the problem of automated grading as
ordinal regression is appropriate. Indeed, as we will discuss, it appears
to be more advantageous to frame the problem as one of ranking
the ungraded assignments, which a human grader can segment  into
any desired grade levels. 

Specifically, as we observed in
Tables~\ref{table:feature-comb}~and~\ref{table:feature-comb-clar}, outright
prediction of an ordinal grade can be very challenging due to the highly
concentrated nature of the dataset labels (see
Table~\ref{table:grade-stats}). The vast majority of grade information
available for the grade prediction task is centered around the mean,
leaving very little information in the tails for a supervised learner to
extract patterns from. (In some cases, for example, there are as few as one
example for the highest and lowest ordinal grade values). The result is
noisy output that may be inappropriate for using directly. However, it is
worth noting that ordinal grade prediction is a hard problem, even for
humans: a previous study suggests disagreement rates around $44\%$ for
short answer grading~\cite{Mohler:2009:EACL}. We suspect that this only
becomes larger as assignments become more complex and difficult to grade,
which makes the task of outright label prediction much more difficult for
the machine as well.

Thus  an alternative, and more reasonable approach may be to produce a ranked list of
assignments from best to worst. Annotators are typically more consistent at
providing judgments of the form ``is $a$ better than $b$?''\ than ``on a
scale from 1--5, how good is $a$?''~\cite{Callison-Burch:2007:WMT}, so it
is reasonable to suspect that a machine learning model could achieve better
results when trained using such pairwise judgments. If a system can provide
a good ranking of assignments, an instructor simply needs to assign
``cutoff'' points in this ranking to determine grades. This simplifies the
learning problem from attempting to predict an ordinal label for a specific
assignment to assigning a ranking to a set of assignments. This is a well
studied area in information retrieval called ``learning to
rank''~\cite{Joachims:2002:KDD, Liu:2009}, and there are a wide variety of methods
available that one can use to learn a ranking function for documents given
a set of available features.  

One particular method that we will explore is a pairwise solution called a
Ranking SVM~\cite{Joachims:2002:KDD}, where the problem of learning to
create ranked lists is decomposed into the problem of determining
preferences for pairs of items (i.e., whether $a$ should appear before
$b$). An traditional SVM model is learned on this decomposition, and its
weight vector is used to define a retrieval function that is the dot
product with a document's feature vector.

Before we explore the efficacy of such an approach, however, we must first
redefine some measure by which we can measure performance. Because the
system is no longer predicting a rating for each assignment, we cannot use
MAE as before.

\subsection{Evaluating Ranking-based Grading Systems}
Our goal is to produce a ranking of student assignments that is consistent
with instructor evaluation. One way of framing this problem is to compare
the ranking produced by the system to the ranking produced by the
instructor (which we'll call the ``reference ranking''). A system's ranking
can then be evaluated using some measure of correlation between the two
rankings. We note a preference for metrics that take into account the
\emph{entire} ranked list---this contrasts with most of the preferred
measures in information retrieval evaluation which typically place heavier
emphasis on the top-ranked elements. While this makes sense in a search
context, our goal is to produce an exhaustive ranking of the assignments,
so we focus on these types of measures.

Measures for rank correlation are plentiful. Perhaps the most commonly
used metrics are Kendall's $\tau$ or Spearman's $\rho$ (which have been
found to be highly correlated in practice~\cite{Shani:2011:Springer}; thus,
we present only one for illustration). Kendall's $\tau$ can be formulated
as
\[
    \tau = \frac{n_c - n_d}{\frac{1}{2} n (n-1)},
\]
where $n_c$ is the number of \emph{concordant pairs}, and $n_d$ is the number
of \emph{discordant pairs}, and $n$ is the number of items ranked. To
compute $n_c$ and $n_d$, one considers all pairs $(x_i, y_i)$ and $(x_j,
y_j)$ (that is, pairs of tuples) of assigned rankings in the system ranking
$X$ and the reference ranking $Y$ (the denominator is simply the number of
such pairs). A pair is \emph{concordant} if the ordering of the items $i$
and $j$ in $X$ and $Y$ is consistent---in other words, if $(x_i < x_j)
\land (y_i < y_j)$ or $(x_i > x_j) \land (y_i > y_j)$.  A pair is
\emph{discordant} if the ordering of items in the two lists is
inconsistent---in other words, if $(x_i < x_j) \land (y_i > y_j)$ or $(x_i
> x_j) \land (y_i < y_j)$. This is then a correlation measure, with values
bounded in $[-1, 1]$, with $1$ indicating a perfect correlation and $-1$
indicating inverse correlation.

One of the assumptions Kendall's $\tau$ makes is that there are no ties in
ranks. However, in a realistic grading scenario based on rubrics we expect
many ties. Fortunately, there is a variation of Kendall's $\tau$, denoted
as $\tau_b$, that accounts for ties in the rankings. This is formulated as
\[
    \tau_b = \frac{n_c - n_d}{\sqrt{(n_c + n_d + t_x)(n_c + n_d + t_y)}}
\]
where $t_x$ is the number of pairs that were tied on \emph{only} their
ranking from $X$, and $t_y$ is the number of pairs that were tied on
\emph{only} their ranking from $Y$.

This may, at first glance then, seem like a good measure to use, but it is
not without its problems. Despite taking into account ties in the rankings,
it may still penalize a system for re-ordering items that were tied in the
reference ranking---in other words, we may be penalized for not correctly
identifying elements who are tied in the reference ranking. Consider a
simple example: suppose the ranking proposed by a system is $X =
(1,2,3,4,5,6)$ but the reference ranking is $Y = (1,1,2,2,3,4)$.
Intuitively, the system made no real mistakes in that no pair where the
reference ranking asserted an is in the wrong order in $X$. However, we'll
see that $\tau_b \approx 0.9309$, indicating that the system did not achieve
perfect correlation.

To address this issue, Yao~\cite{Yao:1995:JASIS} proposed the normalized
distance-based performance measure (NDPM), which computes a distance
between two rankings that is insensitive to a system's reordering of tied
elements in the reference ranking. NDPM is computed as
\[
    NDPM = \frac{2n_d + t_x}{2(n_c + n_d + t_x)}.
\]
This can also be described as the distance between the system ranking and
the reference ranking divided by the maximum achievable distance any
ranking could have from the reference ranking. Thus, a value of $0.3$ would
indicate that the system ranking was 30\% of the distance away from the
reference ranking than the reverse of the reference ranking.  Since this is
a normalized \emph{distance} measure, a value of $0$ would indicate a
perfect ranking. Indeed, if we compute NDPM for the example rankings above,
we achieve this result. Thus, we feel that NDPM is perhaps the most
appropriate measure for evaluating automatic grading systems that produce
an ordering of assignments as their output.

\input{sec-active-learning}

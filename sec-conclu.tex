\section{Discussion and Future Work}

In the previous experiments we formulated the automated grading problem as
a ranking problem, and introduced a rank distance measure (NDPM) as a form
of evaluating the quality of a ranked list generated by an automated (or
semi-automated, in our case) system. Under a ranking-based problem
formulation, we argue that this is the most sensible metric for evaluating
performance relative to a gold standard.

However, the value of NDPM cannot be easily compared with the values of
existing metrics (such as MAE) that have been traditionally used in
evaluating automated grading systems in the past. There is a need to
evaluate a ranked list from the perspective of its impact on the eventual
grades assigned to student work. Unfortunately, how to evaluate the utility
of a ranked list appropriately remains a challenge partly due to the
difficulty in choosing the cutoffs, which may depend on the desired
tradeoff that an instructor wants (e.g., a desired distribution of grades
in different brackets). In practice, we envision that the instructor would
visit points in the ranked list and choose cutoffs based on the tradeoff
between the different types of grading errors. Exploring grade cutoff
assignment strategies remains an important future direction, and our
framework coupled with such a cutoff strategy would enable evaluation based
on the traditional grade prediction task.

While we believe the results here show that the methods employed are
feasible for grading complex assignments, more work remains to be done to
understand just how well our system performs relative to human judgments.
Future work should explore this by measuring human consensus in grading
these complex assignments, similar to what has done for short
answers~\cite{Mohler:2009:EACL}. Furthermore, we only investigated very
simplistic features---such as the bag-of-words model---which are very
general but not very sophisticated. Exploring the feature space further to
find more sophisticated features that perform well in practice and are more
tailored to the goals of medical case assessments remains as future work.

Another major limitation of our study is the limited size of the data set.
This is partly due to the fact that such complex assignments currently can
only be graded by human graders. In the future, we hope to deploy our
automated grading tools to help scale up such courses to enable more
students to participate, which in turn, would help collecting more data for
experiments.

Finally, a crucial direction that remains unexplored is feedback: how could
such a system give more detailed feedback to students beyond just their
ordinal rating along a rubric dimension? Currently, peer grading approaches
have an advantage in this sense, as your peers can suggest to you
corrections or point out specific mistakes that you made. It is worth
investigating whether or not we can generate ``explanatory reports'' of
grading results when using a supervised learning approach.

\section{Conclusions}

Automated grading of complex assignments is necessary for scaling up
learning without compromising effectiveness of learning. Using a data set
of medical case assessment assignments, we conducted the first systematic
study of how we might be able to use machine learning to automate grading
of such a complex assignment. Our study has led to several contributions.

First, we have experimentally shown the feasibility of using supervised
learning techniques for automated grading of medical case assignments under
certain conditions provided that the instructor can manually label a number
of the assignments to serve as a training set. In particular, an ordinal
regression method can be applied to the data with results that consistently
outperform the majority-label baseline in terms of mean absolute error.

Second, we proposed a general framework for the development of three
complementary types of representative features for student submissions
(i.e., token features, similarity features, and selection features)
---while we applied these features to our specific task of medical case
analysis grading, these feature types (and generation framework) are
general and should apply to the grading of any complex assignment.

Third, we proposed to frame the problem of automated grading as a ranking
problem, which can more naturally assist human graders to validate and
finalize grades of ungraded assignments and learn from pairwise preference
judgments that can be potentially created more reliably by human graders
including through peer grading. We also suggested NDPM as potentially a
better measure for this ranking task than other measures due to its
superiority in handling many tied cases.

Finally, we proposed an iterative procedure of online active learning to
rank to efficiently utilize human judgments, and thus optimizing the
collaboration between human graders and the automated grader. Experiment
results confirm the efficiency of this procedure which can substantially
save human effort as compared with randomly choosing sample assignments for
humans to grade.
